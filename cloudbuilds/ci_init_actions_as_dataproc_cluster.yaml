steps:
  # copy init actions to GCS
  - name: 'gcr.io/cloud-builders/gsutil'
    id: 'copy_resources'
    dir: 'dataproc/'
    args:
      - 'cp'
      - '-r'
      - 'init-actions/*.sh'
      - 'gs://${_BUCKET_NAME}/${REPO_NAME}/${BRANCH_NAME}/${SHORT_SHA}/temp/dataproc/init-actoins/'

  # import autoscaling-policy
  - name: 'gcr.io/cloud-builders/gcloud'
    id: 'import_auto_scaling_policy'
    args:
      - 'dataproc'
      - 'autoscaling-policies'
      - 'import'
      - '--region=${_REGION}'
      - 'general-policy-${_REGION}-${SHORT_SHA}'
      - '--source=dataproc/autoscaling/autoscaling-policy.yaml'

  ## create autoscaling dataproc cluster
  - name: 'gcr.io/cloud-builders/gcloud'
    id: 'create_autoscaling_dataproc_cluster'
    args:
      - 'dataproc'
      - 'clusters'
      - 'create'
      - '${_CLUSTER_NAME}'
      - '--region=${_REGION}'
      - '--zone=${_ZONE}'
      - '--master-machine-type=n1-standard-1'
      - '--worker-machine-type=n1-standard-1'
      - '--num-workers=2'
      - '--master-boot-disk-size=15'
      - '--worker-boot-disk-size=15'
      - '--autoscaling-policy=general-policy-${_REGION}-${SHORT_SHA}'
      - '--initialization-actions=gs://${_BUCKET_NAME}/${REPO_NAME}/${BRANCH_NAME}/${SHORT_SHA}/temp/dataproc/init-actoins/add_hive_jars.sh'
      - '--metadata=hive-aux-libs=gs://${_BUCKET_NAME}/${REPO_NAME}/${BRANCH_NAME}/${SHORT_SHA}/hive-aux-libs/'

  ## Run teragen test
  - name: 'gcr.io/cloud-builders/gcloud'
    id: 'run_teragen_test'
    args:
      - 'dataproc'
      - 'jobs'
      - 'submit'
      - 'hadoop'
      - '--cluster=${_CLUSTER_NAME}'
      - '--region=${_REGION}'
      - '--class=org.apache.hadoop.examples.terasort.TeraGen'
      - '--jars=file:///usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar'
      - '--'
      - '100000000'
      - 'gs://${_BUCKET_NAME}/${REPO_NAME}/${BRANCH_NAME}/${SHORT_SHA}/temp/test/output/teragen_output'


  ## Run pi-spark test
  - name: 'gcr.io/cloud-builders/gcloud'
    id: 'run_pi_spark_test'
    args:
      - 'dataproc'
      - 'jobs'
      - 'submit'
      - 'spark'
      - '--cluster=${_CLUSTER_NAME}'
      - '--region=${_REGION}'
      - '--jars=file:///usr/lib/spark/examples/jars/spark-examples.jar'
      - '--class=org.apache.spark.examples.JavaSparkPi'
      - '--'
      - '3'

  ## delete dataproc cluster
  - name: 'gcr.io/cloud-builders/gcloud'
    id: 'delete_autoscaling_dataproc_cluster'
    args:
      - 'dataproc'
      - 'clusters'
      - 'delete'
      - '${_CLUSTER_NAME}'
      - '--region=${_REGION}'

  # copy to work dir
  - name: gcr.io/$PROJECT_ID/manifest-util
    args: ['--git_sha', '$COMMIT_SHA',
           '--git_brunch', '$BRANCH_NAME',
           '--build_id', '$BUILD_ID',
           '--upload' ]
    id: 'set-manifest'
    dir: 'dataproc/'

  ## delete policy
  - name: 'gcr.io/cloud-builders/gcloud'
    id: 'delete_auto_scaling_policy'
    args:
      - 'dataproc'
      - 'autoscaling-policies'
      - 'delete'
      - '--region=${_REGION}'
      - "general-policy-${_REGION}-${SHORT_SHA}"

  # remove temp resources
  - name: 'gcr.io/cloud-builders/gsutil'
    id: 'remove_temp_resources'
    args:
      - 'rm'
      - '-r'
      - 'gs://${_BUCKET_NAME}/${REPO_NAME}/${BRANCH_NAME}/${SHORT_SHA}/temp'