substitutions:
  _BUILD_BUCKET: ${_BUCKET_NAME}/${_REPO_NAME}/${_BRANCH_NAME}/${_SHORT_SHA}

steps:

  # get init_actions itself
  - name: 'gcr.io/cloud-builders/git'
    args: ['clone', 'https://github.com/elisska/gcp-cicd.git', 'dataproc-samples']
    volumes:
      - path: '/dataproc-samples'
        name: 'repository'

  # copy init actions to GCS
  - name: 'gcr.io/cloud-builders/gsutil'
    args:
      - 'cp'
      - '-r'
      - './dataproc-samples/init-actions/*.sh'
      - 'gs://${_BUCKET_NAME}/${_REPO_NAME}/${_BRANCH_NAME}/${_SHORT_SHA}/temp/init-actoins/'
    volumes:
      - path: '/dataproc-samples'
        name: 'repository'

  # import autoscaling-policy
  - name: 'gcr.io/cloud-builders/gcloud'
    args:
      - 'dataproc'
      - 'autoscaling-policies'
      - 'import'
      - '--region=${_REGION}'
      - 'general-policy-${_REGION}-${_SHORT_SHA}'
      - '--source=dataproc-samples/autoscaling/autoscaling-policy.yaml'
    volumes:
      - path: '/dataproc-samples'
        name: 'repository'

  ## create single node autoscaling dataproc cluster
  - name: 'gcr.io/cloud-builders/gcloud'
    args:
      - 'dataproc'
      - 'clusters'
      - 'create'
      - '${_CLUSTER_NAME}'
      - '--region=${_REGION}'
      - '--zone=${_ZONE}'
      - '--master-machine-type=n1-standard-1'
      - '--worker-machine-type=n1-standard-1'
      - '--num-workers=2'
      - '--master-boot-disk-size=15'
      - '--worker-boot-disk-size=15'
      - '--autoscaling-policy=general-policy-${_REGION}-${_SHORT_SHA}'
      - '--initialization-actions=gs://${_BUCKET_NAME}/${_REPO_NAME}/${_BRANCH_NAME}/${_SHORT_SHA}/temp/init-actoins/add_hive_jars.sh'
      - '--metadata=hive-aux-libs=gs://${_BUCKET_NAME}/${_REPO_NAME}/${_BRANCH_NAME}/${_SHORT_SHA}/hive-aux-libs/'
    volumes:
      - path: '/dataproc-samples'
        name: 'repository'

  ## Run teragen test
  - name: 'gcr.io/cloud-builders/gcloud'
    args:
      - 'dataproc'
      - 'jobs'
      - 'submit'
      - 'hadoop'
      - '--cluster=${_CLUSTER_NAME}'
      - '--region=${_REGION}'
      - '--class=org.apache.hadoop.examples.terasort.TeraGen'
      - '--jars=file:///usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar'
      - '--'
      - '1000000'
      - 'gs://${_BUCKET_NAME}/${_REPO_NAME}/${_BRANCH_NAME}/${_SHORT_SHA}/temp/test/output/teragen_output'


  ## Run pi-spark test
  - name: 'gcr.io/cloud-builders/gcloud'
    args:
      - 'dataproc'
      - 'jobs'
      - 'submit'
      - 'spark'
      - '--cluster=${_CLUSTER_NAME}'
      - '--region=${_REGION}'
      - '--jars=file:///usr/lib/spark/examples/jars/spark-examples.jar'
      - '--class=org.apache.spark.examples.JavaSparkPi'
      - '--'
      - '3'

  ## delete single node dataproc cluster
  - name: 'gcr.io/cloud-builders/gcloud'
    args:
      - 'dataproc'
      - 'clusters'
      - 'delete'
      - '${_CLUSTER_NAME}'
      - '--region=${_REGION}'

  ## delete policy
  - name: 'gcr.io/cloud-builders/gcloud'
    args:
      - 'gcloud'
      - 'dataproc'
      - 'autoscaling-policies'
      - 'delete'
      - '--region=${_REGION}'
      - "general-policy-${_REGION}-${_SHORT_SHA}"

  # copy to work dir
  - name: 'gcr.io/cloud-builders/gsutil'
    args:
      - 'cp'
      - '-r'
      - 'gs://${_BUCKET_NAME}/${_REPO_NAME}/${_BRANCH_NAME}/${_SHORT_SHA}/temp/init-actoins'
      - 'gs://${_BUCKET_NAME}/${_REPO_NAME}/${_BRANCH_NAME}/${_SHORT_SHA}/init-actoins'

  # remove temp resources
  - name: 'gcr.io/cloud-builders/gsutil'
    args:
      - 'rm'
      - '-r'
      - 'gs://${_BUCKET_NAME}/${_REPO_NAME}/${_BRANCH_NAME}/${_SHORT_SHA}/temp'
