steps:

  # copy init actions to GCS
  - name: 'gcr.io/cloud-builders/gsutil'
    id: 'copy_resources'
    args:
      - 'cp'
      - '-r'
      - 'dataproc/init-actions/*.sh'
      - 'gs://${_BUCKET_NAME}/$REPO_NAME/$BRANCH_NAME/$COMMIT_SHA/temp/dataproc/init-actoins/'

  ## create dataproc cluster
  - name: 'gcr.io/cloud-builders/gcloud'
    id: 'create dataproc cluster'
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        if [ ${_CLUSTER_TYPE} == 'single-node' ]; then
          echo "create single node dataproc cluster"
          gcloud dataproc clusters create ${_CLUSTER_NAME}-${_CLUSTER_TYPE} \
            --single-node \
            --region=${_REGION} \
            --zone=${_ZONE} \
            --initialization-actions=gs://${_BUCKET_NAME}/${REPO_NAME}/${BRANCH_NAME}/${COMMIT_SHA}/temp/dataproc/init-actoins/add_hive_jars.sh \
            --metadata=hive-aux-libs=gs://${_BUCKET_NAME}/${REPO_NAME}/${BRANCH_NAME}/${COMMIT_SHA}/hive-aux-libs ;
        elif [ ${_CLUSTER_TYPE} == 'multi-node' ]; then
          echo "create multi node dataproc cluster";
          gcloud dataproc clusters create ${_CLUSTER_NAME}-${_CLUSTER_TYPE} \
            --region=${_REGION} \
            --zone=${_ZONE} \
            --num-workers=2 \
            --master-machine-type=n1-standard-1 \
            --worker-machine-type=n1-standard-1 \
            --initialization-actions=gs://${_BUCKET_NAME}/${REPO_NAME}/${BRANCH_NAME}/${COMMIT_SHA}/temp/dataproc/init-actoins/add_hive_jars.sh \
            --metadata=hive-aux-libs=gs://${_BUCKET_NAME}/${REPO_NAME}/${BRANCH_NAME}/${COMMIT_SHA}/hive-aux-libs ;
        elif [ ${_CLUSTER_TYPE} == 'high-availability' ]; then
          echo "create high availability dataproc cluster";
          gcloud dataproc clusters create ${_CLUSTER_NAME}-${_CLUSTER_TYPE} \
            --region=${_REGION} \
            --zone=${_ZONE} \
            --num-masters=3 \
            --num-workers=2 \
            --master-machine-type=n1-standard-1 \
            --worker-machine-type=n1-standard-1 \
            --master-boot-disk-size=15 \
            --worker-boot-disk-size=15 \
            --initialization-actions=gs://${_BUCKET_NAME}/${REPO_NAME}/${BRANCH_NAME}/${COMMIT_SHA}/temp/dataproc/init-actoins/add_hive_jars.sh \
            --metadata=hive-aux-libs=gs://${_BUCKET_NAME}/${REPO_NAME}/${BRANCH_NAME}/${COMMIT_SHA}/hive-aux-libs ;
        elif [ ${_CLUSTER_TYPE} == 'auto-scaling' ]; then
          echo "import autoscaling-policy";
          gcloud dataproc autoscaling-policies import general-policy-${_REGION}-${COMMIT_SHA} \
            --region=${_REGION} \
            --source=dataproc/autoscaling/autoscaling-policy.yaml ;

          echo "create auto scaling dataproc cluster";
          gcloud dataproc clusters create ${_CLUSTER_NAME}-${_CLUSTER_TYPE} \
            --region=${_REGION} \
            --zone=${_ZONE} \
            --master-machine-type=n1-standard-1 \
            --worker-machine-type=n1-standard-1 \
            --num-workers=2 \
            --master-boot-disk-size=15 \
            --worker-boot-disk-size=15 \
            --autoscaling-policy=general-policy-${_REGION}-${COMMIT_SHA} \
            --initialization-actions=gs://${_BUCKET_NAME}/${REPO_NAME}/${BRANCH_NAME}/${COMMIT_SHA}/temp/dataproc/init-actoins/add_hive_jars.sh \
            --metadata=hive-aux-libs=gs://${_BUCKET_NAME}/${REPO_NAME}/${BRANCH_NAME}/${COMMIT_SHA}/hive-aux-libs ;

            echo "delete autoscaling-policy";
            gcloud dataproc autoscaling-policies delete general-policy-${_REGION}-${COMMIT_SHA} \
              --region=${_REGION};
        else
          echo "Specify '_CLUSTER_TYPE'. It must be 'single-node', 'multi-node', 'high-availability' or 'auto-scaling'";
          exit 1;
        fi

  ## Run teragen test
  - name: 'gcr.io/cloud-builders/gcloud'
    id: 'run_teragen_test'
    args:
      - 'dataproc'
      - 'jobs'
      - 'submit'
      - 'hadoop'
      - '--cluster=${_CLUSTER_NAME}-${_CLUSTER_TYPE}'
      - '--region=${_REGION}'
      - '--class=org.apache.hadoop.examples.terasort.TeraGen'
      - '--jars=file:///usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar'
      - '--'
      - '100000000'
      - 'gs://${_BUCKET_NAME}/${REPO_NAME}/${BRANCH_NAME}/${COMMIT_SHA}/temp/test/output/teragen_output'


  ## Run pi-spark test
  - name: 'gcr.io/cloud-builders/gcloud'
    id: 'run_pi_spark_test'
    args:
      - 'dataproc'
      - 'jobs'
      - 'submit'
      - 'spark'
      - '--cluster=${_CLUSTER_NAME}-${_CLUSTER_TYPE}'
      - '--region=${_REGION}'
      - '--jars=file:///usr/lib/spark/examples/jars/spark-examples.jar'
      - '--class=org.apache.spark.examples.JavaSparkPi'
      - '--'
      - '3'

  ## delete single node dataproc cluster
  - name: 'gcr.io/cloud-builders/gcloud'
    id: 'delete_dataproc_cluster'
    args:
      - 'dataproc'
      - 'clusters'
      - 'delete'
      - '${_CLUSTER_NAME}-${_CLUSTER_TYPE}'
      - '--region=${_REGION}'

  # remove temp resources
  - name: 'gcr.io/cloud-builders/gsutil'
    id: 'remove_temp_resources'
    args:
      - 'rm'
      - '-r'
      - 'gs://${_BUCKET_NAME}/${REPO_NAME}/${BRANCH_NAME}/${COMMIT_SHA}/temp'

  # copy to work dir
  - name: gcr.io/$PROJECT_ID/manifest-util
    args: ['--git_sha', '$COMMIT_SHA',
           '--git_branch', '$BRANCH_NAME',
           '--build_id', '$BUILD_ID',
           '--upload' ]
    id: 'set-manifest'
    dir: 'dataproc/'